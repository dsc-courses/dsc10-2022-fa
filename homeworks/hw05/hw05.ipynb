{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Simulation, Sampling, and Hypothesis Testing\n",
    "\n",
    "## Due Tuesday, November 8th at 11:59PM\n",
    "\n",
    "Welcome to Homework 5! This homework will cover:\n",
    "- Simulations (see [CIT 9.3-9.4](https://inferentialthinking.com/chapters/09/3/Simulation.html))\n",
    "- Sampling and Empirical Distributions (see [CIT 10-10.4](https://inferentialthinking.com/chapters/10/Sampling_and_Empirical_Distributions.html))\n",
    "- Models and Hypothesis Testing (see [CIT 11.2](https://inferentialthinking.com/chapters/11/2/Multiple_Categories.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "This assignment is due on Tuesday, November 8th at 11:59PM. You are given six slip days throughout the quarter to extend deadlines. See the syllabus for more details. With the exception of using slip days, late work will not be accepted unless you have made special arrangements with your instructor.\n",
    "\n",
    "**Important**: For homeworks, the `otter` tests don't usually tell you that your answer is correct. More often, they help catch careless mistakes. It's up to you to ensure that your answer is correct. If you're not sure, ask someone (not for the answer, but for some guidance about your approach). These are great questions for office hours (see the schedule on the [Calendar](https://dsc10.com/calendar)) or EdStem. Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please don't change this cell, but do make sure to run it.\n",
    "import babypandas as bpd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import otter\n",
    "grader = otter.Notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "%reload_ext pandas_tutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lucky Triton Lotto, Continued  🔱 🎱 🧜\n",
    "\n",
    "In the last homework, we calculated the probability of winning the grand prize (free housing) on a Lucky Triton Lotto lottery ticket, and found that it was quite low 😭."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell, do not change it!\n",
    "free_housing_chance = (1 / 62) * (1 / 61) * (1 / 60) * (1 / 59) * (1 / 58) * (1 / 16)\n",
    "free_housing_chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we'll approach the same question not using math, but using simulation. \n",
    "\n",
    "It's important to remember how this lottery works:\n",
    "- When you buy a Lucky Triton Lotto ticket, you first pick five different numbers, one at a time, from 1 to 62. Then you separately pick a number from 1 to 16, which may or may not be the same as one of the first five. These are **your numbers**. For example, you may select (15, 1, 13, 3, 61, 8). This is a sequence of six numbers - **order matters**!\n",
    "- The **winning numbers** are chosen by King Triton drawing five balls, one at a time, **without replacement**, from a pot of white balls numbered 1 to 62. Then, he draws a gold ball, the Tritonball, from a pot of gold balls numbered 1 to 16. Both pots are completely separate, hence the different ball colors. For example, maybe the winning numbers are (13, 15, 62, 3, 5, 8).\n",
    "\n",
    "We’ll assume for this problem that in order to win the grand prize (free housing), all six of your numbers need to match the winning numbers and be in the **exact same positions**. In other words, your entire sequence of numbers must be exactly the same. However, if some numbers in your sequence match up with the corresponding number in the winning sequence, you will still win some Triton Cash. \n",
    "\n",
    "Suppose again that your numbers are (15, 1, 13, 3, 61, 8) and the winning numbers are (13, 15, 62, 3, 5, 8). In this case, two of your numbers are considered to match two of the winning numbers. Notice that although both sequences include the number 15 within the first five numbers (representing a white ball), since they are in different positions, that's not considered a match.\n",
    "\n",
    "- Your numbers: (15, 1, 13, **3**, 61, **8**)\n",
    "- Winning numbers: (13, 15, 62, **3**, 5, **8**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1.** Write a function called `simulate_one_ticket`. It should take no arguments, and it should return an array with 6 random numbers, simulating how the numbers are selected for a single Lucky Triton Lotto ticket. The first five numbers should all be randomly chosen without replacement, from 1 to 62. The last number should be between 1 and 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_one_ticket():\n",
    "    \"\"\"Simulate one Lucky Triton Lotto ticket.\"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2.** It's draw day. You checked the winning numbers King Triton drew, which happened to be **(55, 12, 3, 51, 23, 5)**. You didn't win free housing, and you are quite sad.\n",
    "\n",
    "Suppose you want to remind yourself how unlikely it is to win the grand prize. Call the function `simulate_one_ticket` 100,000 times. In your 100,000 tickets, **how many times did you win the grand prize (free housing)?** Assign your answer to `count_free_housing`. (It would cost a fortune if you were to buy 100,000 tickets – it's pretty nice to be able to simulate this experiment instead of doing it in real life!) \n",
    "\n",
    "*Hints*:\n",
    "\n",
    "- First, implement a simulation where you only buy 10 tickets. Once you are sure you have that figured out, change it to 100,000 tickets. It may take a little while (up to a minute) for Python to perform the calculations when you are buying 100,000 tickets.\n",
    "\n",
    "- You'll have to count how many of the numbers you chose match the numbers that were drawn. One way to do this involves [`np.count_nonzero`](https://numpy.org/doc/stable/reference/generated/numpy.count_nonzero.html). Remember you need **all** the numbers to match to win the grand prize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_free_housing = ...\n",
    "...\n",
    "count_free_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the mathematical probability of winning free housing is quite low, on the order of $10^{-11}$. That's a lot lower than than 1 in 100,000, which is $10^{-5}$.\n",
    "\n",
    "**Question 1.3.** As we've seen, you would need to be extremely lucky to win the grand prize. To encourage more students to buy Lucky Triton Lotto tickets, students can win Triton Cash if some of their numbers match the corresponding winning numbers, as described in the introduction. Again, simulate the act of buying 100,000 tickets, but this time find **the greatest number of matches achieved by any of your tickets**, and assign this number to `most_matches`. \n",
    "\n",
    "The winning numbers are the same from the previous part: **(55, 12, 3, 51, 23, 5)**.\n",
    "\n",
    "For example, if 90,000 of your tickets matched 1 winning number and 10,000 of your tickets matched 2 winning numbers, then you would set `most_matches` to 2. If 99,999 of your tickets matched 1 winning number and one of your tickets matched 4 winning numbers, you would set `most_matches` to 4. If you happened to win the grand prize on one of your tickets, you would set `most_matches` to 6. Remember, order matters.\n",
    "\n",
    "*Hint*: There are several ways to approach this; one way involves storing the number of matches per ticket in an array and finding the largest number in that array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_matches = ...\n",
    "...\n",
    "most_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4.** Suppose one Lucky Triton Lotto ticket costs $2.\n",
    "\n",
    "The Lucky Triton Lotto advertisement on Instagram promises you will never lose money because of the following generous prizes:\n",
    "\n",
    "- Win $10 with a 1-number match\n",
    "\n",
    "- Win $25 with a 2-number match\n",
    "\n",
    "- Win $100 with a 3-number match\n",
    "\n",
    "- Win $1,000 with a 4-number match\n",
    "\n",
    "- Win $5,000 with a 5-number match\n",
    "\n",
    "- Win $20,000 with a 6-number match (free housing!)\n",
    "\n",
    "If you had the money to buy 100,000 tickets, what would be your net winnings from buying these tickets? Since this is net winnings, this should account for the prizes you win and the cost of buying the tickets. Assign the amount to `net_winnings`. Note that a positive value means you won money overall, and a negative value means you lost money overall. Do you believe the advertisement's claims?\n",
    "\n",
    "The winning numbers are the same from the previous part: **(55, 12, 3, 51, 23, 5)**.\n",
    "\n",
    "*Hint*: Again, there are a few ways you could approach this problem. One way involves generating another 100,000 random tickets and counting the amount earned per ticket, adding to a running total. Alternatively, if you created an array of the number of matches per ticket in Question 1.3, you could loop through that array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_winnings = ...\n",
    "...\n",
    "net_winnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sampling with Netflix 🍿\n",
    "\n",
    "In this question, we will use a dataset consisting of information about all Netflix Original movies to get some practice with sampling. Run the cell below to load the data into a DataFrame, indexed by title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell, do not change it!\n",
    "movie_data = bpd.read_csv('data/netflix_originals.csv').set_index('Title')\n",
    "movie_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a function called `compute_statistics` that takes as input a DataFrame with two columns, `'Runtime'` and `'IMDb Score'`, and then:\n",
    "- draws a histogram of `'Runtime'`,\n",
    "- draws a histogram of `'IMDb Score'`, and\n",
    "- returns a two-element array containing the mean `'Runtime'` and mean `'IMDb Score'`.\n",
    "\n",
    "Run the cell below to define the `compute_statistics` function, and a helper function called `histograms`. Don't worry about how this code works, and please don't change anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this cell, just run it.\n",
    "def histograms(df):\n",
    "    runtimes = df.get('Runtime').values\n",
    "    ratings = df.get('IMDb Score').values\n",
    "    \n",
    "    plt.subplots(1, 2, figsize=(15, 4), dpi=100)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(runtimes, density=True, alpha=0.5, color='blue', ec='w', bins=np.arange(0, 250, 10))\n",
    "    plt.title('Distribution of Runtimes')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(ratings, density=True, alpha=0.5, color='blue', ec='w', bins=np.arange(0, 10, 0.4))\n",
    "    plt.title('Distribution of IMDb Scores')\n",
    "    \n",
    "def compute_statistics(runtimes_and_ratings_data, draw=True):\n",
    "    if draw:\n",
    "        histograms(runtimes_and_ratings_data)\n",
    "    avg_runtime = np.average(runtimes_and_ratings_data.get('Runtime').values)\n",
    "    avg_rating = np.average(runtimes_and_ratings_data.get('IMDb Score').values)\n",
    "    avg_array = np.array([avg_runtime, avg_rating]) \n",
    "    return avg_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this `compute_statistics` function to show the distribution of `'Runtime'` and `'IMDb Score'` and compute their means, for any collection of movies. \n",
    "\n",
    "Run the next cell to show these distributions and compute the means for all Netflix Original movies. Notice that an array containing the mean `'Runtime'` and mean `'IMDb Score'` values is displayed before the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_stats = compute_statistics(movie_data)\n",
    "movie_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that instead of having access to the full *population* of movies, we only have access to data on a smaller subset of movies, or a *sample*.  For 584 movies, it's not so unreasonable to expect to see all the data, but usually we aren't so lucky.  Instead, we often make *statistical inferences* about a large underlying population using a smaller sample.\n",
    "\n",
    "**Statistical inference** is the process of using data in a sample to _infer_ some characteristic about the population from which the sample was drawn. A common strategy for statistical inference is to estimate parameters of the population by computing the same statistics on a sample. This strategy sometimes works well and sometimes doesn't.  The degree to which it gives us useful answers depends on several factors.\n",
    "\n",
    "One very important factor in the utility of samples is how they were gathered. Let's look at some different sampling strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convenience sampling\n",
    "One sampling methodology, which is **generally a bad idea**, is to choose movies which are somehow convenient to sample.  For example, you might choose movies that you have personally watched, since it's easier to collect information about them.  This is called, somewhat pejoratively, *convenience sampling*.\n",
    "\n",
    "**Question 2.1.**  Suppose you love scary movies 👻 and you decide to manually look up information on all Netflix Original movies in the following genres:\n",
    "- `'Horror'`\n",
    "- `'Thriller'`\n",
    "- `'Horror thriller'`\n",
    "\n",
    "Assign `convenience_sample` to a subset of `movie_data` that contains only the rows for movies that are in one of these genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convenience_sample = ...\n",
    "convenience_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2.** Assign `convenience_stats` to an array of the mean `'Runtime'` and mean `'IMDb Score'` of your convenience sample.  Since they're computed on a sample, these are called *sample means*. \n",
    "\n",
    "**_Hint_**: Use the function `compute_statistics`; it's okay if histograms are displayed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convenience_stats = ...\n",
    "convenience_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll compare the distribution of `'Runtime'` in our convenience sample to the distribution of `'Runtime'` for all the movies in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell, do not change it!\n",
    "def compare_runtimes(first, second, first_title, second_title):\n",
    "    \"\"\"Compare the runtimes in two DataFrames.\"\"\"\n",
    "    bins = np.arange(0, 250, 10)\n",
    "    \n",
    "    plt.subplots(1, 2, figsize=(15, 4), dpi=85)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(first.get('Runtime'), bins=bins, density=True, ec='w', color='blue', alpha=0.5)\n",
    "    plt.title(f'Runtimes ({first_title})')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(second.get('Runtime'), bins=bins, density=True, ec='w', color='blue', alpha=0.5)\n",
    "    plt.title(f'Runtimes ({second_title})')\n",
    "\n",
    "compare_runtimes(movie_data, convenience_sample, 'All Movies', 'Convenience Sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3.** From what you see in the histograms above, did the convenience sample give us an accurate picture of the runtimes for the full population of movies?  Why or why not?\n",
    "\n",
    "Assign either 1, 2, 3, or 4 to the variable `sampling_q3` below. \n",
    "1. Yes. The sample is large enough, so it is an accurate representation of the population.\n",
    "2. No. Normally convenience samples give us an accurate representation of the population, but only if the sample size is large enough. Our convenience sample here was too small.\n",
    "3. No. Normally convenience samples give us an accurate representation of the population, but we just got unlucky.\n",
    "4. No. Convenience samples generally don't give us an accurate representation of the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_q3 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple random sampling\n",
    "A more principled approach is to sample uniformly at random from the movies.  If we ensure that each movie is selected at most once, this is a **random sample without replacement**, sometimes abbreviated to \"**simple random sample**\" or \"**SRS**\".  Imagine writing down each movie's title on a card, putting the cards in a hat, and shuffling the hat.  To sample, pull out cards one by one and set them aside, stopping when the specified *sample size* is reached.\n",
    "\n",
    "We've produced two simple random samples of `ratings_data`: the variable `small_srs_data` contains a SRS of size 70, and the variable `large_srs_data` contains a SRS of size 180."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run the same analyses on the small simple random sample, the large simple random sample, and the convenience sample. The subsequent code draws the histograms and computes the means for `'Runtime'` and `'IMDb Score'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this cell, but do run it.\n",
    "small_srs_data = bpd.read_csv('data/small_srs_rating.csv').set_index('Title')\n",
    "large_srs_data = bpd.read_csv('data/large_srs_rating.csv').set_index('Title')\n",
    "\n",
    "small_stats = compute_statistics(small_srs_data, draw=False);\n",
    "large_stats = compute_statistics(large_srs_data, draw=False);\n",
    "convenience_stats = compute_statistics(convenience_sample, draw=False);\n",
    "\n",
    "print('Full data stats:                 ', movie_stats)\n",
    "print('Small SRS stats:', small_stats)\n",
    "print('Large SRS stats:', large_stats)\n",
    "print('Convenience sample stats:        ', convenience_stats)\n",
    "\n",
    "color_dict = {\n",
    "    'small SRS': 'blue',\n",
    "    'large SRS': 'green',\n",
    "    'convenience sample': 'orange'\n",
    "}\n",
    "\n",
    "plt.subplots(3, 2, figsize=(15, 15), dpi=100)\n",
    "i = 1\n",
    "\n",
    "for df, name in zip([small_srs_data, large_srs_data, convenience_sample], color_dict.keys()):\n",
    "    plt.subplot(3, 2, i)\n",
    "    i += 2\n",
    "    plt.hist(df.get('Runtime'), density=True, alpha=0.5, color=color_dict[name], ec='w', \n",
    "             bins=np.arange(0, 250, 10))\n",
    "    plt.title(f'Runtimes ({name})');\n",
    "\n",
    "i = 2\n",
    "for df, name in zip([small_srs_data, large_srs_data, convenience_sample], color_dict.keys()):\n",
    "    plt.subplot(3, 2, i)\n",
    "    i += 2\n",
    "    plt.hist(df.get('IMDb Score'), density=True, alpha=0.5, color=color_dict[name], ec='w', \n",
    "             bins=np.arange(0, 10, 0.4))\n",
    "    plt.title(f'IMDb Ratings ({name})');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing simple random samples\n",
    "Often it's useful to take random samples even when we have a larger dataset available.  One reason is that doing so can help us understand how inaccurate other samples are.\n",
    "\n",
    "As we saw in Lecture 14, DataFrames have a `.sample` method for producing simple random samples.  Note that its default is to sample **without** replacement, which aligns with how simple random samples are drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4.** Produce a simple random sample *without replacement* of size 70 from `movie_data`. Store an array containing the mean `'Runtime'` and mean `'IMDb Score'` of your SRS in `my_small_stats`. Again, it's fine if histograms are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_small_stats = ...\n",
    "my_small_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell in which `my_small_stats` is defined many times, to collect new samples and compute their sample means.\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, recall, `small_stats` is an array containing the mean `'Runtime'` and mean `'IMDb Score'` for the one small SRS that we provided you with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Answer the following two-fold question:\n",
    "- Are the values in `my_small_stats` (the mean `'Runtime'` and `'IMDb Score'` for **your** small SRS) similar to the values in `small_stats` (the mean `'Runtime'` and `'IMDb Score'` for the small SRS **we provided you with**)? \n",
    "- Each time you collect a new sample – i.e. each time you re-run the cell where `my_small_stats` is defined – do the values in `my_small_stats` change a lot?\n",
    "\n",
    "Assign either 1, 2, 3, or 4 to the variable `sampling_q4` below.\n",
    "1. The values in `my_small_stats` are identical to the values in `small_stats`, and change a bit each time a new sample is collected.\n",
    "2. The values in `my_small_stats` are identical to the values in `small_stats`, and don't change at all each time a new sample is collected.\n",
    "3. The values in `my_small_stats` are very different from the values in `small_stats`, and don't change at all each time a new sample is collected.\n",
    "4. The values in `my_small_stats` are slightly different from the values in `small_stats`, and change a bit each time a new sample is collected.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_q4 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5.** Similarly, create a simple random sample of size 180 from `movie_data` and store an array of the sample's mean `'Runtime'` and mean `'IMDb Score'` in `my_large_stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_large_stats = ...\n",
    "my_large_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell in which `my_large_stats` is defined many times. Do the histograms and  mean statistics (mean `'Runtime'` and mean `'IMDb Score'`) seem to change more or less across samples of size 180 than across samples of size 70?\n",
    "\n",
    "Assign either 1, 2, or 3 to the variable `sampling_q5` below. \n",
    "\n",
    "1. The statistics change *less* across samples of size 180 than across samples of size 70.\n",
    "2. The statistics change an *equal amount* across samples of size 180 and across samples of size 70.\n",
    "3. The statistics change *more* across samples of size 180 than across samples of size 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_q5 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Was it by Random Chansey? 🎲\n",
    "\n",
    "<img src='data/chansey.png' width='250'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You recently decided to buy the video game *Pokémon Yellow* from someone on Ebay. The seller tells you that they've modified the game so that the probabilities of encountering certain Pokémon in certain locations have been altered. However, the seller doesn't tell you which specific locations have had their probability models changed and what they've been changed to.\n",
    "\n",
    "As you are playing *Pokémon Yellow*, you arrive at the Safari Zone, one of the most iconic locations in the game. You're curious as to your chances of encountering your favorite Pokémon, Chansey, in this location. You go onto [Bulbapedia](https://bulbapedia.bulbagarden.net/wiki/Kanto_Safari_Zone#Area_1) to find the probability model for this location, and you discover that for each Pokémon encounter in the Safari Zone, there is a 4% chance of encountering Chansey. \n",
    "\n",
    "After a few hours of gameplay in the Safari Zone, you have encountered Chansey only 23 times out of 784 total Pokémon encounters (around 2.9%). You start to suspect that the Safari Zone may have been one of the locations in which the previous owner of the game changed the probability model.\n",
    "\n",
    "To test this, you decide to run a hypothesis test with the following hypotheses:\n",
    "\n",
    "**Null Hypothesis**: In your copy of *Pokémon Yellow*, the probability of encountering Chansey at each Pokémon encounter in the Safari Zone is 4%. \n",
    "\n",
    "**Alternative Hypothesis**: In your copy of *Pokémon Yellow*, the probability of encountering Chansey at each Pokémon encounter in the Safari Zone is less than 4%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1.** Complete the implementation of the function `one_simulation`, which has no arguments. It should randomly generate 784 Pokémon encounters in the Safari Zone and return the **proportion** of encountered Pokémon that were Chansey. \n",
    "\n",
    "*Hint*: Use `np.random.multinomial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_simulation():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2.** The test statistic for our hypothesis test will be the difference between the proportion of Chansey encounters in a given sample of 784 Safari Zone encounters and the expected proportion of Chansey encounters, i.e.\n",
    "\n",
    "$$\\text{test statistic} = \\text{proportion of Chansey encounters in sample} - 0.04$$\n",
    "\n",
    "\n",
    "Let's conduct 10,000 simulations. Create an array named `proportion_diffs` containing 10,000 simulated values of the test statistic described above. Utilize the function created in the previous question to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_diffs = ...\n",
    "\n",
    "# Visualize with a histogram. Don't change anything below.\n",
    "bpd.DataFrame().assign(absolute_differences=proportion_diffs).plot(kind='hist', bins=20, density=True, ec='w', figsize=(10, 5));\n",
    "plt.axvline(x=(23 / 784 - 0.04), color='black', linewidth=4, label='observed statistic')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3.** Calculate the p-value for this hypothesis test, and assign the result to `safari_zone_p`.\n",
    "\n",
    "*Hint*: Do large values of our test statistic favor the alternative hypothesis, or do small values of our test statistic favor the alternative hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safari_zone_p = ...\n",
    "safari_zone_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4.** Using the standard p-value cutoff of 0.05, what can we conclude from our hypothesis test? Assign either 1, 2, 3, or 4 to the variable `safari_zone_conclusion`, corresponding to the best conclusion.\n",
    "   \n",
    "   1. We reject the null hypothesis. There is not enough evidence to say whether the observed data is consistent with the model.\n",
    "   1. We reject the null hypothesis. The observed data is inconsistent with the model.\n",
    "   1. We accept the null hypothesis. The observed data is consistent with the model.\n",
    "   1. We fail to reject the null hypothesis. There is not enough evidence to say that the observed data is inconsistent with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safari_zone_conclusion = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.5.** In this question, we chose as our test statistic the proportion of Chansey encounters in the Safari Zone minus 0.04. But this is not the only statistic we could have chosen; there are many that could have worked here. \n",
    "\n",
    "From the options below, choose the test statistic that would **not** have worked for this hypothesis test, and assign 1, 2, 3, or 4 to the variable `bad_choice`.\n",
    "\n",
    "1. The number of Chansey encounters out of 784 enounters in the Safari Zone.\n",
    "1. The proportion of Chansey encounters in the Safari Zone.\n",
    "1. 0.04 minus the proportion of Chansey encounters in the Safari Zone.\n",
    "1. The absolute difference between 0.04 and the proportion of Chansey encounters in the Safari Zone.\n",
    "\n",
    "*Hint*: Our goal is to find a test statistic that will help us determine whether we encounter Chansey **less** often than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_choice = ...\n",
    "bad_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <span style='color:#FF1480'> Surprise Mini Brands!</span>  🍭🧴🩹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you buy a Surprise Mini Brands toy, you open it up to reveal tiny replicas of branded supermarket products. Here are some of the possible items you may see when opening a Surprise Mini Brands toy:\n",
    "<img src='data/minibrand.png' width='650'>\n",
    "\n",
    "No, that is not real pasta sauce!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four types of replicas in a Surprise Mini Brands toy: `'Gold'`, `'Metallic'`, `'Glow in the Dark'`, and `'Common'`. The first three are \"rare\" types, which are made of special materials.\n",
    "\n",
    "Unfortunately, Zuru, the company behind Surprise Mini Brands, doesn't make public the probability of getting any of the four types of replicas. A DSC 10 tutor proposed the following probability distribution:\n",
    "\n",
    "| Type | Estimated Probability of Type |\n",
    "| --- | --- |\n",
    "| Gold | $\\frac{1}{15}$ |\n",
    "| Metallic | $\\frac{1}{15}$ |\n",
    "| Glow in the Dark | $\\frac{1}{30}$ |\n",
    "| Common | $\\frac{5}{6}$ |\n",
    "\n",
    "We'll store this distribution in an array, in the order `'Gold'`, `'Metallic'`, `'Glow in the Dark'`, and `'Common'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell, do not change it!\n",
    "type_distribution_tutor = np.array([1 / 15, 1 / 15, 1 / 30, 5 / 6])\n",
    "type_distribution_tutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the validity of their model, the tutor surveyed many individuals who purchased Surprise Mini Brands toys and asked them for the types of replicas they received. In total, they were given information about 15,525 replicas, out of which:\n",
    "- 818 were `'Gold'`,\n",
    "- 976 were `'Metallic'`,\n",
    "- 412 were `'Glow in the Dark'`, and\n",
    "- the rest were `'Common'`.\n",
    "\n",
    "We can calculate the **empirical** type distribution using survey data and store it in an array as well (in the same order as before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell, do not change it!\n",
    "empirical_type_distribution = np.array([818, 976, 412, (15525 - 818 - 976 - 412)]) / 15525\n",
    "empirical_type_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `empirical_type_distribution` is not identical to `type_distribution_tutor`, it's still possible that the tutor's model is plausible, and that the observed differences are due to random chance. Let's run a hypothesis test to investigate further, using the following hypotheses: \n",
    "\n",
    "**Null Hypothesis**: The types of Surprise Mini Brands toys are drawn randomly from the distribution `type_distribution_tutor`.\n",
    "\n",
    "**Alternative Hypothesis**: The types of Surprise Mini Brands toys are _not_ drawn randomly from the distribution `type_distribution_tutor`.\n",
    "\n",
    "Note that this hypothesis test involves four proportions – one for each of `'Gold'`, `'Metallic'`, `'Glow in the Dark'`, and `'Common'`.\n",
    "\n",
    "**Question 4.1.**  Which of the following is **not** a reasonable choice of test statistic for this hypothesis test? Assign 1, 2, or 3 to the variable `unreasonable_test_statistic`. \n",
    "1. The total variation distance between the proposed distribution (expected proportion of types) and the empirical distribution (actual proportion of types).\n",
    "2. The sum of the absolute difference between the proposed distribution (expected proportion of types) and the empirical distribution (actual proportion of types).\n",
    "3. The absolute difference between the sum of the proposed distribution (expected proportion of types) and the sum of the empirical distribution (actual proportion of types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unreasonable_test_statistic = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2.** We'll use the TVD, i.e. **total variation distance**, as our test statistic. Below, complete the implementation of the function `total_variation_distance`, which takes in two distributions (stored as arrays) as arguments and returns the total variation distance between the two arrays.\n",
    "\n",
    "Then, use the function `total_variation_distance` to determine the TVD between the type distribution proposed by the tutor and the empirical type distribution observed. Assign this TVD to `observed_tvd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_distance(first_distrib, second_distrib):\n",
    "    '''Computes the total variation distance between two distributions.'''\n",
    "    ...\n",
    "\n",
    "observed_tvd = ...\n",
    "observed_tvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.3.** Now, we'll calculate 5,000 simulated TVDs to see what a typical TVD between the proposed distribution and an empirical distribution would look like if the tutor's model were accurate. Since our real-life data includes 15,525 replicas, in each trial of the simulation, we'll:\n",
    "- draw 15,525 replicas at random from the tutor's proposed distribution, then \n",
    "- calculate the TVD between the **type distribution proposed by the tutor** and the **empirical type distribution from the simulated sample**. \n",
    "\n",
    "Store these 5,000 simulated TVDs in an array called `simulated_tvds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_tvds = ...\n",
    "\n",
    "# Visualize the distribution of TVDs with a histogram\n",
    "bpd.DataFrame().assign(simulated_tvds=simulated_tvds).plot(kind='hist', density=True, ec='w', figsize=(10, 5));\n",
    "plt.axvline(x=observed_tvd, color='black', linewidth=4, label='observed TVD')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.4.** Now, check the p-value of our test by computing the proportion of times in our simulation that we saw a TVD greater than or equal to our observed TVD. Assign your result to `type_p_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_p_value = ...\n",
    "type_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.5.** Using the standard p-value cutoff of 0.05, what can we conclude from our hypothesis test? Assign either 1, 2, 3, or 4 to the variable `type_conclusion`, corresponding to the best conclusion.\n",
    "   \n",
    "   1. We reject the null hypothesis. There is not enough evidence to say whether the observed data is consistent with the model.\n",
    "   1. We reject the null hypothesis. The observed data is inconsistent with the model.\n",
    "   1. We accept the null hypothesis. The observed data is consistent with the model.\n",
    "   1. We fail to reject the null hypothesis. There is not enough evidence to say that the observed data is inconsistent with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_conclusion = ...\n",
    "type_conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish Line 🏁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit your assignment:\n",
    "\n",
    "1. Select `Kernel -> Restart & Run All` to ensure that you have executed all cells, including the test cells.\n",
    "2. Read through the notebook to make sure everything is fine and all tests passed.\n",
    "3. Run the cell below to run all tests, and make sure that they all pass.\n",
    "4. Download your notebook using `File -> Download as -> Notebook (.ipynb)`, then upload your notebook to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
